<?xml version="1.0"?>
<Container version="2">
	<Name>open-webui</Name>
	<Repository>ghcr.io/open-webui/open-webui:main</Repository>
	<Registry>https://github.com/open-webui/open-webui/pkgs/container/open-webui</Registry>
	<Branch>
	<Tag>main</Tag>
		<TagDescription>Standard container</TagDescription>
	</Branch>
	<Branch>
		<Tag>main-slim</Tag>
		<TagDescription>Slim verion without built in models</TagDescription>
	</Branch>
	<Network>bridge</Network>
	<MyIP/>
	<Shell>bash</Shell>
	<Privileged>false</Privileged>
	<Support>https://github.com/open-webui/open-webui/issues</Support>
	<Project>https://github.com/open-webui/open-webui</Project>
	<Overview>
ChatGPT-Style Web Interface for various LLM runners, including Ollama and OpenAI-compatible APIs.

Configuration:
https://docs.openwebui.com/getting-started/env-configuration

There are a lot of missing environment options missing from the template but can add more if anyone wants.
	</Overview>
	<Category>AI: Tools: Network:Web</Category>
	<WebUI>http://[IP]:[PORT:8080]/</WebUI>
	<TemplateURL>https://raw.githubusercontent.com/davidjkling/unraid-templates/main/templates/open-webui.xml</TemplateURL>
	<Icon>https://raw.githubusercontent.com/davidjkling/unraid-templates/refs/heads/main/static/open-webui.png</Icon>
	<ExtraParams/>
	<PostArgs/>
	<CPUset/>
	<DonateText/>
	<DonateLink/>
	<Requires/>
	<Config Name="Data" Target="/app/backend/data" Default="/mnt/user/appdata/openwebui" Mode="rw" Description="" Type="Path" Display="always" Required="true" Mask="false">/mnt/user/appdata/openwebui</Config>
	<Config Name="WebUI Port" Target="8080" Default="8080" Mode="tcp" Description="Sets the port to run Open WebUI from." Type="Port" Display="always" Required="true" Mask="false">8080</Config>
	<Config Name="ENABLE_OLLAMA_API" Target="ENABLE_OLLAMA_API" Default="true|false" Mode="" Description="Enables the use of Ollama APIs." Type="Variable" Display="always" Required="false" Mask="false">true</Config>
	<Config Name="OLLAMA_BASE_URL" Target="OLLAMA_BASE_URL" Default="http://localhost:11434" Mode="" Description="Configures the Ollama backend URL." Type="Variable" Display="always" Required="false" Mask="false">http://localhost:11434</Config>
	<Config Name="ENABLE_OPENAI_API" Target="ENABLE_OPENAI_API" Default="true|false" Mode="" Description="Enables the use of OpenAI APIs." Type="Variable" Display="always" Required="false" Mask="false">true</Config>
	<Config Name="OPENAI_API_BASE_URL" Target="OPENAI_API_BASE_URL" Default="https://api.openai.com/v1" Mode="" Description="Configures the OpenAI base API URL." Type="Variable" Display="always" Required="false" Mask="false">https://api.openai.com/v1</Config>
	<Config Name="OPENAI_API_KEY" Target="OPENAI_API_KEY" Default="" Mode="" Description="Sets the OpenAI API key, e.g. sk-124781258123." Type="Variable" Display="always" Required="false" Mask="true" />
	<Config Name="WEBUI_AUTH" Target="WEBUI_AUTH" Default="true|false" Mode="" Description="This setting enables or disables authentication." Type="Variable" Display="advanced" Required="false" Mask="false">true</Config>
	<Config Name="GLOBAL_LOG_LEVEL" Target="GLOBAL_LOG_LEVEL" Default="DEBUG|INFO|WARNING|ERROR|CRITICAL" Mode="" Description="Sets the global logging level for all Open WebUI components. " Type="Variable" Display="advanced" Required="false" Mask="false">INFO</Config>
	<Config Name="ENABLE_EVALUATION_ARENA_MODELS" Target="ENABLE_EVALUATION_ARENA_MODELS" Default="true|false" Mode="" Description="Enables or disables evaluation arena models." Type="Variable" Display="advanced" Required="false" Mask="false">true</Config>
	<Config Name="ENABLE_MESSAGE_RATING" Target="ENABLE_MESSAGE_RATING" Default="true|false" Mode="" Description="Enables message rating feature." Type="Variable" Display="advanced" Required="false" Mask="false">true</Config>
	<Config Name="ENABLE_COMMUNITY_SHARING" Target="ENABLE_COMMUNITY_SHARING" Default="true|false" Mode="" Description="Enables message rating feature." Type="Variable" Display="advanced" Required="false" Mask="false">true</Config>
	<Config Name="HTTP_PROXY" Target="HTTP_PROXY" Default="" Mode="" Description="Proxy for HTTP." Type="Variable" Display="advanced" Required="false" Mask="false" />
	<Config Name="HTTPS_PROXY" Target="HTTPS_PROXY" Default="" Mode="" Description="Proxy for HTTPS." Type="Variable" Display="advanced" Required="false" Mask="false" />
	<Config Name="NO_PROXY" Target="NO_PROXY" Default="" Mode="" Description="Comma-separate list of hosts/IPs that bypass the proxy." Type="Variable" Display="advanced" Required="false" Mask="false" />
</Container>
